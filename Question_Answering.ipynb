{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17793,
     "status": "ok",
     "timestamp": 1610633987215,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "xLKQCb3q4wtA",
    "outputId": "ba67a596-0896-456a-8fff-a15d3f138ccb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\mini-projet\\python.exe\n",
      "C:\\Users\\RACHID AHSOUNE\\AppData\\Local\\Programs\\Python\\Python37\\python.exe\n",
      "C:\\Users\\RACHID AHSOUNE\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\n"
     ]
    }
   ],
   "source": [
    "!where python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4176,
     "status": "ok",
     "timestamp": 1610633992701,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "qpBVHa3a458Z",
    "outputId": "6cc8e36e-100b-4bb9-e10f-0326f70cdd36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"kernelspecs\": {\n",
      "    \"anaconda-mini-projet\": {\n",
      "      \"resource_dir\": \"C:\\\\Users\\\\RACHID AHSOUNE\\\\AppData\\\\Roaming\\\\jupyter\\\\kernels\\\\anaconda-mini-projet\",\n",
      "      \"spec\": {\n",
      "        \"argv\": [\n",
      "          \"C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\mini-projet\\\\python.exe\",\n",
      "          \"-m\",\n",
      "          \"ipykernel_launcher\",\n",
      "          \"-f\",\n",
      "          \"{connection_file}\"\n",
      "        ],\n",
      "        \"env\": {},\n",
      "        \"display_name\": \"Anaconda (mini-projet)\",\n",
      "        \"language\": \"python\",\n",
      "        \"interrupt_mode\": \"signal\",\n",
      "        \"metadata\": {}\n",
      "      }\n",
      "    },\n",
      "    \"python3\": {\n",
      "      \"resource_dir\": \"c:\\\\programdata\\\\anaconda3\\\\envs\\\\mini-projet\\\\share\\\\jupyter\\\\kernels\\\\python3\",\n",
      "      \"spec\": {\n",
      "        \"argv\": [\n",
      "          \"python\",\n",
      "          \"-m\",\n",
      "          \"ipykernel_launcher\",\n",
      "          \"-f\",\n",
      "          \"{connection_file}\"\n",
      "        ],\n",
      "        \"env\": {},\n",
      "        \"display_name\": \"Python 3\",\n",
      "        \"language\": \"python\",\n",
      "        \"interrupt_mode\": \"signal\",\n",
      "        \"metadata\": {}\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!jupyter kernelspec list --json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python 3.6.9 :: Anaconda, Inc.\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYTxg7qC4ws9"
   },
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wsn9KrrTwCYV"
   },
   "source": [
    "### Download and Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56vO4ShhwQXJ"
   },
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 775,
     "status": "ok",
     "timestamp": 1610633996918,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "u0n4Kkku5SRg"
   },
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def read_data(file_path, max_context_size = 100):\n",
    "  # Read dataset\n",
    "  with open(file_path,encoding=\"utf-8\") as f:\n",
    "      data = json.load(f)\n",
    "\n",
    "  contexts = []\n",
    "  questions = []\n",
    "  answers = []\n",
    "  labels = []\n",
    "  \n",
    "  for i in range(len(data['data'])):\n",
    "\n",
    "    paragraph_object = data['data'][i][\"paragraphs\"]\n",
    "    \n",
    "    for j in range(len(paragraph_object)):\n",
    "\n",
    "      context_object = paragraph_object[j]\n",
    "      context_text = context_object['context']\n",
    "\n",
    "      if len(context_text.split()) > max_context_size:\n",
    "        continue\n",
    "      for k in range(len(context_object['qas'])):\n",
    "\n",
    "        question_object = context_object['qas'][k]\n",
    "        question_text = question_object['question']\n",
    "        \n",
    "        answer_object = question_object['answers'][0]\n",
    "        answer_text = answer_object['text']\n",
    "        answer_start = answer_object['answer_start']\n",
    "        answer_end = answer_start + len(answer_text)\n",
    "\n",
    "        answer_start = len(context_text[:answer_start].split())\n",
    "        answer_end = answer_start + len(answer_text.split())\n",
    "        if answer_end >= max_context_size:\n",
    "          answer_end = max_context_size -1\n",
    "        labels.append([answer_start, answer_end])\n",
    "        questions.append(question_text)\n",
    "        contexts.append(context_text)\n",
    "        answers.append(answer_text)\n",
    "        \n",
    "  with open('train_contexts.txt', 'w',encoding=\"utf-8\") as f:\n",
    "    f.write(('\\n').join(contexts))\n",
    "  with open('train_questions.txt', 'w' ,encoding=\"utf-8\") as f:\n",
    "    f.write(('\\n').join(questions))\n",
    "  # with open('questAnswers.txt',\"w\") as file:\n",
    "  #   file.write(('\\n').join(questAnswers))\n",
    "  return {'qas':questions, 'ctx':contexts, 'ans':answers, 'lbl':labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1978,
     "status": "ok",
     "timestamp": 1610634003365,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "qVxr4zyBrK45"
   },
   "outputs": [],
   "source": [
    "train_data = read_data('test-context-ar-question-ar.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 797,
     "status": "ok",
     "timestamp": 1610634005692,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "fjya3QAVZJf7",
    "outputId": "1a44b4c6-c158-46c4-cb42-18b3a3cd64d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ما الذي جعل شريط الاختبار للطائرة؟\n",
      "بحيرة جرووم كانت تستخدم للقصف المدفعي والتدريب علي المدفعية خلال الحرب العالمية الثانية، ولكن تم التخلي عنها بعد ذلك حتى نيسان / أبريل 1955، عندما تم اختياره من قبل فريق لوكهيد اسكنك كموقع مثالي لاختبار لوكهيد يو-2 - 2 طائرة التجسس. قاع البحيرة قدم الشريط المثالية التي يمكن عمل اختبارات الطائرات المزعجة، ارتفاع سلسلة جبال وادي الإيمجرانت ومحيط NTS يحمي موقع الاختبار من أعين المتطفلين والتدخل الخارجي.\n",
      "قاع البحيرة\n",
      "==============\n",
      "من كان يرافق طائرة يو -2 عند التسليم؟\n",
      "شيدت لوكهيد قاعدة مؤقتة في الموقع، ثم عرفت باسم الموقع الثاني أو \"المزرعة\"، التي تتألف من أكثر بقليل من بضعة مخابئ، وحلقات عمل ومنازل متنقلة لفريقها الصغير. في ثلاثة أشهر فقط شيد مدرج طوله 5000  ودخل الخدمة بحلول تموز / يوليو 1955. حصلت المزرعة على تسليم أول يو 2 في 24 يوليو، 1955 من بوربانك على سي 124 جلوب ماستر الثاني طائرة شحن، يرافقه فنيي وكهيد على دي سي 3. انطلق أول يو - 2 من الجرووم في 4 أغسطس، 1955. بدأت عمليات تحليق أسطول يو 2 تحت سيطرة وكالة المخابرات المركزية الأمريكية في الأجواء السوفياتية بحلول منتصف عام 1956.\n",
      "فنيي وكهيد\n",
      "==============\n",
      " ما هو نوع العمل الذي يواجهه الطيارون العسكريون إذا انتقلوا إلى \\n مناطق محظورة؟ \n",
      "على عكس الكثير من حدود نيليس، والمنطقة المحيطة بها في البحيرة بشكل دائم خارج الحدود سواء على المدنيين وطبيعية حركة الطيران العسكري. محطات الرادار لحماية المنطقة، والأفراد غير مصرح بها سرعان ما تطرد. حتى طيارين التدريب العسكري في خطر NAFR إجراءات التأديبية إذا تواجدوا بطريق الخطأ في \"المربع\"الحظور للجرووم والأجواء المحيطة بها.\n",
      "إجراءات التأديبية\n",
      "==============\n",
      "متى تم نشر مقال مجلة الطيران؟\n",
      "في كانون الثاني 2006، نشر مؤرخ الفضاء دواين أ يوم مقال نشر في المجلة الإلكترونية الطيران والفضاء استعراض بعنوان \"رواد الفضاء والمنطقة 51 : حادث سكايلاب\". المقال كان مبنيا على مذكرة مكتوبة في عام 1974 إلى مديروكالة المخابرات المركزية يام كولبي من قبل عملاء مجهولين لوكالة الاستخبارات المركزية. وذكرت المذكرة أن رواد الفضاء على متن سكايلاب 4، وذلك كجزء من برنامج أوسع نطاقا، عن غير قصد بالتقاط صور لموقع الذي قالت المذكرة :\n",
      "كانون الثاني 2006\n",
      "==============\n",
      "ما هو الموقع الذي أصبح مركزاً للأطباق الطائرة ونظريات المؤامرة؟\n",
      "لطبيعتها السرية وفيما لا شك فيه بحوث تصنيف الطائرات، إلى جانب تقارير عن الظواهر غير العادية، قد أدت الي ان تصبح منطقة 51 مركزا للاطباق الطائرة الحديثة ونظريات المؤامرة. بعض الأنشطة المذكورة في مثل هذه النظريات في منطقة 51 تشمل ما يلي :\n",
      "منطقة 51\n",
      "==============\n",
      "ما كان محور مؤامرة الجسم الغريب الحديثة؟\\n\n",
      "لطبيعتها السرية وفيما لا شك فيه بحوث تصنيف الطائرات، إلى جانب تقارير عن الظواهر غير العادية، قد أدت الي ان تصبح منطقة 51 مركزا للاطباق الطائرة الحديثة ونظريات المؤامرة. بعض الأنشطة المذكورة في مثل هذه النظريات في منطقة 51 تشمل ما يلي :\n",
      "منطقة 51\n",
      "==============\n",
      "مالذي يُظن بأنه قد تم بنائه في روزويل؟\n",
      "التخزين، والفحص، والهندسة العكسية للمركبة الفضائية الغريبة المحطمة (بما في ذلك مواد يفترض ان تعافى في روزويل)، ودراسة شاغليها (حية أو ميتة)، وصناعة الطائرات على أساس التكنولوجيا الغريبة.\n",
      "صناعة الطائرات على أساس التكنولوجيا الغريبة\n",
      "==============\n",
      "متى يقوم Qos بالتفاوض على كيفية عمل الشبكة؟\n",
      "ويمكن أن تتوافق الشبكة أو البروتوكول الذي يدعم جودة الخدمات على عقد المرور مع تطبيق البرمجيات والقدرة الاحتياطية في عقد الشبكة، على سبيل المثال خلال مرحلة إقامة الدورات. وهي يمكن أن تحقق رصدا لمستوى الأداء خلال الدورة، على سبيل المثال معدل البيانات والتأخير، والتحكم ديناميكيا عن طريق جدولة الأولويات في عقد الشبكة. وقد تفرج عن القدرة الاحتياطية خلال مرحلة الهدم.\n",
      "مرحلة إقامة الدورات\n",
      "==============\n",
      "ما هو أحد الشروط للتجارة الشبكية المتنوعة؟\n",
      "جودة الخدمة قد تكون مطلوبة لأنواع معينة من حركة مرور الشبكة، على سبيل المثال :\n",
      "جودة الخدمة\n",
      "==============\n",
      "كم عدد قوائم الانتظار الموجودة على أجهزة توجيه المختلفة؟\\n\n",
      "الموجهات لدعم DiffServ استخدام قوائم متعددة للحزم في انتظار انتقال من عرض النطاق الترددي مقيدة (على سبيل المثال، منطقة واسعة) واجهات. راوتر الباعة يوفر قدرات مختلفة لتكوين هذا السلوك، لتشمل عددا من قوائم معتمدة، والأولويات النسبية لقوائم الانتظار، وعرض النطاق الترددي المخصصة لكل قائمة انتظار.\n",
      "متعددة\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "  print(train_data['qas'][i])\n",
    "  print(train_data['ctx'][i])\n",
    "  print(train_data['ans'][i])\n",
    "  print(\"==============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfA0kQYBwdGl"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 4067,
     "status": "ok",
     "timestamp": 1610634015554,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "QrFV7p7f4wtC"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import numpy as np\n",
    "import tkseem as tk\n",
    "from flask import Flask, render_template, request\n",
    "# from flask_ngrok import run_with_ngrok\n",
    "# import pyarabic.araby as araby\n",
    "# from __future__ import unicode_literals\n",
    "import tensorflow as tf\n",
    "import keras.models\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsKGssPu4wtH"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27045,
     "status": "ok",
     "timestamp": 1610634045256,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "wJyd7eXI4wtI",
    "outputId": "a878b38a-ca09-4b13-b72a-8627450ab4a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WordTokenizer ...\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 84: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b8b9f6d20802>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mqa_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWordTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mqa_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_questions.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# print(qa_tokenizer)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\mini-projet\\lib\\site-packages\\tkseem\\word_tokenizer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, file_path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training WordTokenizer ...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_truncate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tokens_frequency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\mini-projet\\lib\\site-packages\\tkseem\\_base.py\u001b[0m in \u001b[0;36m_get_tokens_frequency\u001b[1;34m(self, file_path)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mdict\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mfrequency\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \"\"\"\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0mtokens_frequency\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\mini-projet\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 84: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "qa_tokenizer = tk.WordTokenizer()\n",
    "qa_tokenizer.train('train_questions.txt')\n",
    "# print(qa_tokenizer)\n",
    "\n",
    "\n",
    "cx_tokenizer = tk.WordTokenizer()\n",
    "\n",
    "cx_tokenizer.train('train_contexts.txt')\n",
    "# print(cx_tokenizer)\n",
    "\n",
    "\n",
    "train_inp_data = qa_tokenizer.encode_sentences(train_data['qas'])\n",
    "# print(train_inp_data)\n",
    "\n",
    "train_tar_data = cx_tokenizer.encode_sentences(train_data['ctx'])\n",
    "# print(train_tar_data)\n",
    "train_tar_lbls = train_data['lbl']\n",
    "train_inp_data.shape, train_tar_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB9izdqz4wtK"
   },
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 6059,
     "status": "ok",
     "timestamp": 1610634052415,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "TTPgEJ_o4wtL"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_inp_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c2bb5f3c4ec2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mBUFFER_SIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_inp_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_inp_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_tar_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_tar_lbls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBUFFER_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_inp_data' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = len(train_inp_data)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_inp_data, train_tar_data, train_tar_lbls)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPIRHUAmw5qH"
   },
   "source": [
    "### Create Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 680,
     "status": "ok",
     "timestamp": 1610634055005,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "28dhyDseWdGj"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output = self.gru(x, initial_state = hidden)\n",
    "        return output\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "  \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, output_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_sz = output_sz\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=False,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc11 = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.fc12 = tf.keras.layers.Dense(output_sz)\n",
    "\n",
    "        self.fc21 = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.fc22 = tf.keras.layers.Dense(output_sz)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x = self.gru(x, initial_state = hidden)\n",
    "        x1 = self.fc11(x)\n",
    "        x2 = self.fc21(x)\n",
    "\n",
    "        x1 = self.fc12(x1)\n",
    "        x2 = self.fc22(x2)\n",
    "        return [x1, x2]\n",
    "\n",
    "def loss_fn(true, pred):\n",
    "  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "  return (cross_entropy(true[:,0:1], pred[0]) + cross_entropy(true[:,1:2], pred[1]))/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPlRQTWix3sd"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1347,
     "status": "ok",
     "timestamp": 1610634061285,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "ruIcHwa1W2VF"
   },
   "outputs": [],
   "source": [
    "units = 1024\n",
    "embedding_dim = 256\n",
    "max_length_inp = train_inp_data.shape[1]\n",
    "max_length_tar = train_tar_data.shape[1]\n",
    "vocab_tar_size = cx_tokenizer.vocab_size\n",
    "vocab_inp_size = qa_tokenizer.vocab_size\n",
    "steps_per_epoch = len(train_inp_data) // BATCH_SIZE\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, max_length_tar)\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "optim = tf.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 685,
     "status": "ok",
     "timestamp": 1610634068375,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "g5X5NylUzJss"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optim)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 961,
     "status": "ok",
     "timestamp": 1610634070462,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "WI4sFFY1zjA1"
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2144613,
     "status": "ok",
     "timestamp": 1610636235496,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "Y6R5--spXGJc",
    "outputId": "298916bd-ff23-4d2e-a346-8782c8e23281"
   },
   "outputs": [],
   "source": [
    "epochs = 60\n",
    "# print(start_epoch)\n",
    "for epoch in range(start_epoch,epochs):\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  epoch_loss = 0\n",
    "  \n",
    "  for idx, (inp, tar, true) in enumerate(dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hidden = encoder(inp, enc_hidden)\n",
    "        pred = decoder(tar, hidden)\n",
    "        loss  = loss_fn(true, pred)\n",
    "    variables = decoder.trainable_variables + encoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optim.apply_gradients(zip(gradients, variables))\n",
    "    epoch_loss += loss.numpy()\n",
    "    if epoch % 5 == 0:\n",
    "      ckpt_manager.save()\n",
    "  print(f\"Epoch {epoch} loss: {epoch_loss/steps_per_epoch:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhHVvsnO4wtW"
   },
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1482,
     "status": "ok",
     "timestamp": 1610637065164,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "ArmMI9GTH9ow"
   },
   "outputs": [],
   "source": [
    "\n",
    "encoder.save_weights(\"encoder_weights\",save_format='tf')\n",
    "decoder.save_weights(\"decoder_weights\",save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1324,
     "status": "ok",
     "timestamp": 1610637173093,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "sKBQLURa4wtW"
   },
   "outputs": [],
   "source": [
    "def answer(question_txt,context_txt):\n",
    "    question = qa_tokenizer.encode_sentences([question_txt], out_length = max_length_inp)\n",
    "    context = cx_tokenizer.encode_sentences([context_txt], out_length = max_length_tar)\n",
    "    question = tf.convert_to_tensor(question)\n",
    "    context = tf.convert_to_tensor(context)\n",
    "    \n",
    "    # encoder.load_weights(\"/content/drive/MyDrive/Colab Notebooks/encoder_weights\")\n",
    "    # decoder.load_weights(\"/content/drive/MyDrive/Colab Notebooks/decoder_weights\")\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_hidden = encoder(question, hidden)\n",
    "    pred = decoder(context, enc_hidden)\n",
    "\n",
    "    start = tf.argmax(pred[0], axis = -1).numpy()[0]\n",
    "    end = tf.argmax(pred[1], axis = -1).numpy()[0]\n",
    "    \n",
    "    if start >= len(context_txt.split()):\n",
    "      start = len(context_txt.split()) - 1\n",
    "    if end >= len(context_txt.split()):\n",
    "      end = len(context_txt.split()) - 1\n",
    "    \n",
    "    # if one word prediction\n",
    "    if end == start:\n",
    "      end += 1\n",
    "    answer_txt = (' ').join(context_txt.split()[start:end])\n",
    "    return answer_txt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 877,
     "status": "ok",
     "timestamp": 1610637177628,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "gsCCZpn77-P-"
   },
   "outputs": [],
   "source": [
    "\n",
    "def response(userQuestion):\n",
    "    qst=open('train_questions.txt', 'r')\n",
    "    cnt=open('train_contexts.txt', 'r')\n",
    "    # word = u\"الْعَرَبِيَّةُ\"\n",
    "\n",
    "    # if araby.is_arabicword(word):\n",
    "    #    print(\"valid arabic\")\n",
    "    # else: \n",
    "    #    print (\"invalid arabic word\")\n",
    "    # araby.strip_tashkeel(word)\n",
    "    # araby.strip_tatweel(word)\n",
    "    # print(word)\n",
    "    res=\"\"\n",
    "    question = qst.readlines()\n",
    "    for i in range(len(question)):\n",
    "       question[i].replace(\"\\n\",'')\n",
    "      #  print(question[i])\n",
    "       if userQuestion in question[i]:\n",
    "             cont=train_data['ctx'][i]\n",
    "            #  print(cont)\n",
    "             res=answer(userQuestion,cont)\n",
    "             break    \n",
    "    print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 697,
     "status": "ok",
     "timestamp": 1610467808775,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "1f2Kg3faEesi",
    "outputId": "2cd6371e-7e2a-4d1f-b797-90dc43bf51b6"
   },
   "outputs": [],
   "source": [
    "response(\"من كان يرافق طائرة يو؟\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_nh2LUQguo8L",
    "outputId": "eb10b87e-f977-4f50-eb56-a335dab7f8e9"
   },
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "# run_with_ngrok(app)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route(\"/get\")\n",
    "def get_bot_reponse():\n",
    "    userText = request.args.get('msg')\n",
    "    print(userText)\n",
    "    print(str(response(userText)))\n",
    "    return str(response(userText))\n",
    "if __name__ == '__main__':\n",
    "  app.run()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Question_Answering.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Anaconda (mini-projet)",
   "language": "python",
   "name": "anaconda-mini-projet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
