{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17793,
     "status": "ok",
     "timestamp": 1610633987215,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "xLKQCb3q4wtA",
    "outputId": "ba67a596-0896-456a-8fff-a15d3f138ccb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install flask_ngrok\n",
    "# !pip install tkseem\n",
    "!python --version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4176,
     "status": "ok",
     "timestamp": 1610633992701,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "qpBVHa3a458Z",
    "outputId": "6cc8e36e-100b-4bb9-e10f-0326f70cdd36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarabic\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/e2/46728ec2f6fe14970de5c782346609f0636262c0941228f363710903aaa1/PyArabic-0.6.10.tar.gz (108kB)\n",
      "\r",
      "\u001b[K     |███                             | 10kB 20.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 20kB 24.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 30kB 21.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 40kB 19.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 51kB 20.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 61kB 16.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 71kB 16.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 81kB 17.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 92kB 15.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 102kB 16.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 112kB 16.6MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyarabic\n",
      "  Building wheel for pyarabic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyarabic: filename=PyArabic-0.6.10-cp36-none-any.whl size=113324 sha256=24d816c9c08de5cd677d988621ea9b262f6fcf73388c0c5b10ebc2daf2612f49\n",
      "  Stored in directory: /root/.cache/pip/wheels/10/b8/f5/b7c1a50e6efb83544844f165a9b134afe7292585465e29b61d\n",
      "Successfully built pyarabic\n",
      "Installing collected packages: pyarabic\n",
      "Successfully installed pyarabic-0.6.10\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyarabic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYTxg7qC4ws9"
   },
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wsn9KrrTwCYV"
   },
   "source": [
    "### Download and Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56vO4ShhwQXJ"
   },
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 775,
     "status": "ok",
     "timestamp": 1610633996918,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "u0n4Kkku5SRg"
   },
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def read_data(file_path, max_context_size = 100):\n",
    "  # Read dataset\n",
    "  with open(file_path,encoding=\"utf-8\") as f:\n",
    "      data = json.load(f)\n",
    "\n",
    "  contexts = []\n",
    "  questions = []\n",
    "  answers = []\n",
    "  labels = []\n",
    "  \n",
    "  for i in range(len(data['data'])):\n",
    "\n",
    "    paragraph_object = data['data'][i][\"paragraphs\"]\n",
    "    \n",
    "    for j in range(len(paragraph_object)):\n",
    "\n",
    "      context_object = paragraph_object[j]\n",
    "      context_text = context_object['context']\n",
    "\n",
    "      if len(context_text.split()) > max_context_size:\n",
    "        continue\n",
    "      for k in range(len(context_object['qas'])):\n",
    "\n",
    "        question_object = context_object['qas'][k]\n",
    "        question_text = question_object['question']\n",
    "        \n",
    "        answer_object = question_object['answers'][0]\n",
    "        answer_text = answer_object['text']\n",
    "        answer_start = answer_object['answer_start']\n",
    "        answer_end = answer_start + len(answer_text)\n",
    "\n",
    "        answer_start = len(context_text[:answer_start].split())\n",
    "        answer_end = answer_start + len(answer_text.split())\n",
    "        if answer_end >= max_context_size:\n",
    "          answer_end = max_context_size -1\n",
    "        labels.append([answer_start, answer_end])\n",
    "        questions.append(question_text)\n",
    "        contexts.append(context_text)\n",
    "        answers.append(answer_text)\n",
    "        \n",
    "  with open('train_contexts.txt', 'w',encoding=\"utf-8\") as f:\n",
    "    f.write(('\\n').join(contexts))\n",
    "  with open('train_questions.txt', 'w' ,encoding=\"utf-8\") as f:\n",
    "    f.write(('\\n').join(questions))\n",
    "  # with open('questAnswers.txt',\"w\") as file:\n",
    "  #   file.write(('\\n').join(questAnswers))\n",
    "  return {'qas':questions, 'ctx':contexts, 'ans':answers, 'lbl':labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1978,
     "status": "ok",
     "timestamp": 1610634003365,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "qVxr4zyBrK45"
   },
   "outputs": [],
   "source": [
    "train_data = read_data('test-context-ar-question-ar.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 797,
     "status": "ok",
     "timestamp": 1610634005692,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "fjya3QAVZJf7",
    "outputId": "1a44b4c6-c158-46c4-cb42-18b3a3cd64d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ما الذي جعل شريط الاختبار للطائرة؟\n",
      "بحيرة جرووم كانت تستخدم للقصف المدفعي والتدريب علي المدفعية خلال الحرب العالمية الثانية، ولكن تم التخلي عنها بعد ذلك حتى نيسان / أبريل 1955، عندما تم اختياره من قبل فريق لوكهيد اسكنك كموقع مثالي لاختبار لوكهيد يو-2 - 2 طائرة التجسس. قاع البحيرة قدم الشريط المثالية التي يمكن عمل اختبارات الطائرات المزعجة، ارتفاع سلسلة جبال وادي الإيمجرانت ومحيط NTS يحمي موقع الاختبار من أعين المتطفلين والتدخل الخارجي.\n",
      "قاع البحيرة\n",
      "==============\n",
      "من كان يرافق طائرة يو -2 عند التسليم؟\n",
      "شيدت لوكهيد قاعدة مؤقتة في الموقع، ثم عرفت باسم الموقع الثاني أو \"المزرعة\"، التي تتألف من أكثر بقليل من بضعة مخابئ، وحلقات عمل ومنازل متنقلة لفريقها الصغير. في ثلاثة أشهر فقط شيد مدرج طوله 5000  ودخل الخدمة بحلول تموز / يوليو 1955. حصلت المزرعة على تسليم أول يو 2 في 24 يوليو، 1955 من بوربانك على سي 124 جلوب ماستر الثاني طائرة شحن، يرافقه فنيي وكهيد على دي سي 3. انطلق أول يو - 2 من الجرووم في 4 أغسطس، 1955. بدأت عمليات تحليق أسطول يو 2 تحت سيطرة وكالة المخابرات المركزية الأمريكية في الأجواء السوفياتية بحلول منتصف عام 1956.\n",
      "فنيي وكهيد\n",
      "==============\n",
      " ما هو نوع العمل الذي يواجهه الطيارون العسكريون إذا انتقلوا إلى \\n مناطق محظورة؟ \n",
      "على عكس الكثير من حدود نيليس، والمنطقة المحيطة بها في البحيرة بشكل دائم خارج الحدود سواء على المدنيين وطبيعية حركة الطيران العسكري. محطات الرادار لحماية المنطقة، والأفراد غير مصرح بها سرعان ما تطرد. حتى طيارين التدريب العسكري في خطر NAFR إجراءات التأديبية إذا تواجدوا بطريق الخطأ في \"المربع\"الحظور للجرووم والأجواء المحيطة بها.\n",
      "إجراءات التأديبية\n",
      "==============\n",
      "متى تم نشر مقال مجلة الطيران؟\n",
      "في كانون الثاني 2006، نشر مؤرخ الفضاء دواين أ يوم مقال نشر في المجلة الإلكترونية الطيران والفضاء استعراض بعنوان \"رواد الفضاء والمنطقة 51 : حادث سكايلاب\". المقال كان مبنيا على مذكرة مكتوبة في عام 1974 إلى مديروكالة المخابرات المركزية يام كولبي من قبل عملاء مجهولين لوكالة الاستخبارات المركزية. وذكرت المذكرة أن رواد الفضاء على متن سكايلاب 4، وذلك كجزء من برنامج أوسع نطاقا، عن غير قصد بالتقاط صور لموقع الذي قالت المذكرة :\n",
      "كانون الثاني 2006\n",
      "==============\n",
      "ما هو الموقع الذي أصبح مركزاً للأطباق الطائرة ونظريات المؤامرة؟\n",
      "لطبيعتها السرية وفيما لا شك فيه بحوث تصنيف الطائرات، إلى جانب تقارير عن الظواهر غير العادية، قد أدت الي ان تصبح منطقة 51 مركزا للاطباق الطائرة الحديثة ونظريات المؤامرة. بعض الأنشطة المذكورة في مثل هذه النظريات في منطقة 51 تشمل ما يلي :\n",
      "منطقة 51\n",
      "==============\n",
      "ما كان محور مؤامرة الجسم الغريب الحديثة؟\\n\n",
      "لطبيعتها السرية وفيما لا شك فيه بحوث تصنيف الطائرات، إلى جانب تقارير عن الظواهر غير العادية، قد أدت الي ان تصبح منطقة 51 مركزا للاطباق الطائرة الحديثة ونظريات المؤامرة. بعض الأنشطة المذكورة في مثل هذه النظريات في منطقة 51 تشمل ما يلي :\n",
      "منطقة 51\n",
      "==============\n",
      "مالذي يُظن بأنه قد تم بنائه في روزويل؟\n",
      "التخزين، والفحص، والهندسة العكسية للمركبة الفضائية الغريبة المحطمة (بما في ذلك مواد يفترض ان تعافى في روزويل)، ودراسة شاغليها (حية أو ميتة)، وصناعة الطائرات على أساس التكنولوجيا الغريبة.\n",
      "صناعة الطائرات على أساس التكنولوجيا الغريبة\n",
      "==============\n",
      "متى يقوم Qos بالتفاوض على كيفية عمل الشبكة؟\n",
      "ويمكن أن تتوافق الشبكة أو البروتوكول الذي يدعم جودة الخدمات على عقد المرور مع تطبيق البرمجيات والقدرة الاحتياطية في عقد الشبكة، على سبيل المثال خلال مرحلة إقامة الدورات. وهي يمكن أن تحقق رصدا لمستوى الأداء خلال الدورة، على سبيل المثال معدل البيانات والتأخير، والتحكم ديناميكيا عن طريق جدولة الأولويات في عقد الشبكة. وقد تفرج عن القدرة الاحتياطية خلال مرحلة الهدم.\n",
      "مرحلة إقامة الدورات\n",
      "==============\n",
      "ما هو أحد الشروط للتجارة الشبكية المتنوعة؟\n",
      "جودة الخدمة قد تكون مطلوبة لأنواع معينة من حركة مرور الشبكة، على سبيل المثال :\n",
      "جودة الخدمة\n",
      "==============\n",
      "كم عدد قوائم الانتظار الموجودة على أجهزة توجيه المختلفة؟\\n\n",
      "الموجهات لدعم DiffServ استخدام قوائم متعددة للحزم في انتظار انتقال من عرض النطاق الترددي مقيدة (على سبيل المثال، منطقة واسعة) واجهات. راوتر الباعة يوفر قدرات مختلفة لتكوين هذا السلوك، لتشمل عددا من قوائم معتمدة، والأولويات النسبية لقوائم الانتظار، وعرض النطاق الترددي المخصصة لكل قائمة انتظار.\n",
      "متعددة\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "  print(train_data['qas'][i])\n",
    "  print(train_data['ctx'][i])\n",
    "  print(train_data['ans'][i])\n",
    "  print(\"==============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfA0kQYBwdGl"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4067,
     "status": "ok",
     "timestamp": 1610634015554,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "QrFV7p7f4wtC"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import numpy as np\n",
    "import tkseem as tk\n",
    "from flask import Flask, render_template, request\n",
    "from flask_ngrok import run_with_ngrok\n",
    "import pyarabic.araby as araby\n",
    "# from __future__ import unicode_literals\n",
    "import tensorflow as tf\n",
    "import keras.models\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsKGssPu4wtH"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27045,
     "status": "ok",
     "timestamp": 1610634045256,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "wJyd7eXI4wtI",
    "outputId": "a878b38a-ca09-4b13-b72a-8627450ab4a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WordTokenizer ...\n",
      "Training WordTokenizer ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2936, 23), (2936, 100))"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_tokenizer = tk.WordTokenizer()\n",
    "qa_tokenizer.train('train_questions.txt')\n",
    "# print(qa_tokenizer)\n",
    "\n",
    "\n",
    "cx_tokenizer = tk.WordTokenizer()\n",
    "\n",
    "cx_tokenizer.train('train_contexts.txt')\n",
    "# print(cx_tokenizer)\n",
    "\n",
    "\n",
    "train_inp_data = qa_tokenizer.encode_sentences(train_data['qas'])\n",
    "# print(train_inp_data)\n",
    "\n",
    "train_tar_data = cx_tokenizer.encode_sentences(train_data['ctx'])\n",
    "# print(train_tar_data)\n",
    "train_tar_lbls = train_data['lbl']\n",
    "train_inp_data.shape, train_tar_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB9izdqz4wtK"
   },
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 6059,
     "status": "ok",
     "timestamp": 1610634052415,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "TTPgEJ_o4wtL"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = len(train_inp_data)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_inp_data, train_tar_data, train_tar_lbls)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPIRHUAmw5qH"
   },
   "source": [
    "### Create Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 680,
     "status": "ok",
     "timestamp": 1610634055005,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "28dhyDseWdGj"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output = self.gru(x, initial_state = hidden)\n",
    "        return output\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "  \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, output_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_sz = output_sz\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=False,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc11 = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.fc12 = tf.keras.layers.Dense(output_sz)\n",
    "\n",
    "        self.fc21 = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.fc22 = tf.keras.layers.Dense(output_sz)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x = self.gru(x, initial_state = hidden)\n",
    "        x1 = self.fc11(x)\n",
    "        x2 = self.fc21(x)\n",
    "\n",
    "        x1 = self.fc12(x1)\n",
    "        x2 = self.fc22(x2)\n",
    "        return [x1, x2]\n",
    "\n",
    "def loss_fn(true, pred):\n",
    "  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "  return (cross_entropy(true[:,0:1], pred[0]) + cross_entropy(true[:,1:2], pred[1]))/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPlRQTWix3sd"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 1347,
     "status": "ok",
     "timestamp": 1610634061285,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "ruIcHwa1W2VF"
   },
   "outputs": [],
   "source": [
    "units = 1024\n",
    "embedding_dim = 256\n",
    "max_length_inp = train_inp_data.shape[1]\n",
    "max_length_tar = train_tar_data.shape[1]\n",
    "vocab_tar_size = cx_tokenizer.vocab_size\n",
    "vocab_inp_size = qa_tokenizer.vocab_size\n",
    "steps_per_epoch = len(train_inp_data) // BATCH_SIZE\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, max_length_tar)\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "optim = tf.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 685,
     "status": "ok",
     "timestamp": 1610634068375,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "g5X5NylUzJss"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optim)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 961,
     "status": "ok",
     "timestamp": 1610634070462,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "WI4sFFY1zjA1"
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2144613,
     "status": "ok",
     "timestamp": 1610636235496,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "Y6R5--spXGJc",
    "outputId": "298916bd-ff23-4d2e-a346-8782c8e23281"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 4.390\n",
      "Epoch 1 loss: 4.264\n",
      "Epoch 2 loss: 4.233\n",
      "Epoch 3 loss: 4.100\n",
      "Epoch 4 loss: 3.971\n",
      "Epoch 5 loss: 3.814\n",
      "Epoch 6 loss: 3.687\n",
      "Epoch 7 loss: 3.562\n",
      "Epoch 8 loss: 3.465\n",
      "Epoch 9 loss: 3.398\n",
      "Epoch 10 loss: 3.343\n",
      "Epoch 11 loss: 3.231\n",
      "Epoch 12 loss: 3.148\n",
      "Epoch 13 loss: 3.072\n",
      "Epoch 14 loss: 2.966\n",
      "Epoch 15 loss: 2.836\n",
      "Epoch 16 loss: 2.716\n",
      "Epoch 17 loss: 2.638\n",
      "Epoch 18 loss: 2.515\n",
      "Epoch 19 loss: 2.391\n",
      "Epoch 20 loss: 2.208\n",
      "Epoch 21 loss: 2.110\n",
      "Epoch 22 loss: 1.956\n",
      "Epoch 23 loss: 1.766\n",
      "Epoch 24 loss: 1.551\n",
      "Epoch 25 loss: 1.347\n",
      "Epoch 26 loss: 1.128\n",
      "Epoch 27 loss: 0.997\n",
      "Epoch 28 loss: 0.840\n",
      "Epoch 29 loss: 0.709\n",
      "Epoch 30 loss: 0.592\n",
      "Epoch 31 loss: 0.518\n",
      "Epoch 32 loss: 0.387\n",
      "Epoch 33 loss: 0.281\n",
      "Epoch 34 loss: 0.220\n",
      "Epoch 35 loss: 0.207\n",
      "Epoch 36 loss: 0.176\n",
      "Epoch 37 loss: 0.150\n",
      "Epoch 38 loss: 0.138\n",
      "Epoch 39 loss: 0.120\n",
      "Epoch 40 loss: 0.082\n",
      "Epoch 41 loss: 0.071\n",
      "Epoch 42 loss: 0.062\n",
      "Epoch 43 loss: 0.059\n",
      "Epoch 44 loss: 0.053\n",
      "Epoch 45 loss: 0.048\n",
      "Epoch 46 loss: 0.053\n",
      "Epoch 47 loss: 0.053\n",
      "Epoch 48 loss: 0.046\n",
      "Epoch 49 loss: 0.065\n",
      "Epoch 50 loss: 0.036\n",
      "Epoch 51 loss: 0.030\n",
      "Epoch 52 loss: 0.030\n",
      "Epoch 53 loss: 0.041\n",
      "Epoch 54 loss: 0.085\n",
      "Epoch 55 loss: 0.145\n",
      "Epoch 56 loss: 0.141\n",
      "Epoch 57 loss: 0.167\n",
      "Epoch 58 loss: 0.152\n",
      "Epoch 59 loss: 0.107\n"
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "# print(start_epoch)\n",
    "for epoch in range(start_epoch,epochs):\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  epoch_loss = 0\n",
    "  \n",
    "  for idx, (inp, tar, true) in enumerate(dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hidden = encoder(inp, enc_hidden)\n",
    "        pred = decoder(tar, hidden)\n",
    "        loss  = loss_fn(true, pred)\n",
    "    variables = decoder.trainable_variables + encoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optim.apply_gradients(zip(gradients, variables))\n",
    "    epoch_loss += loss.numpy()\n",
    "    if epoch % 5 == 0:\n",
    "      ckpt_manager.save()\n",
    "  print(f\"Epoch {epoch} loss: {epoch_loss/steps_per_epoch:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhHVvsnO4wtW"
   },
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 1482,
     "status": "ok",
     "timestamp": 1610637065164,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "ArmMI9GTH9ow"
   },
   "outputs": [],
   "source": [
    "\n",
    "encoder.save_weights(\"encoder_weights\",save_format='tf')\n",
    "decoder.save_weights(\"decoder_weights\",save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 1324,
     "status": "ok",
     "timestamp": 1610637173093,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "sKBQLURa4wtW"
   },
   "outputs": [],
   "source": [
    "def answer(question_txt,context_txt):\n",
    "    question = qa_tokenizer.encode_sentences([question_txt], out_length = max_length_inp)\n",
    "    context = cx_tokenizer.encode_sentences([context_txt], out_length = max_length_tar)\n",
    "    question = tf.convert_to_tensor(question)\n",
    "    context = tf.convert_to_tensor(context)\n",
    "    \n",
    "    # encoder.load_weights(\"/content/drive/MyDrive/Colab Notebooks/encoder_weights\")\n",
    "    # decoder.load_weights(\"/content/drive/MyDrive/Colab Notebooks/decoder_weights\")\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_hidden = encoder(question, hidden)\n",
    "    pred = decoder(context, enc_hidden)\n",
    "\n",
    "    start = tf.argmax(pred[0], axis = -1).numpy()[0]\n",
    "    end = tf.argmax(pred[1], axis = -1).numpy()[0]\n",
    "    \n",
    "    if start >= len(context_txt.split()):\n",
    "      start = len(context_txt.split()) - 1\n",
    "    if end >= len(context_txt.split()):\n",
    "      end = len(context_txt.split()) - 1\n",
    "    \n",
    "    # if one word prediction\n",
    "    if end == start:\n",
    "      end += 1\n",
    "    answer_txt = (' ').join(context_txt.split()[start:end])\n",
    "    return answer_txt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 877,
     "status": "ok",
     "timestamp": 1610637177628,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "gsCCZpn77-P-"
   },
   "outputs": [],
   "source": [
    "\n",
    "def response(userQuestion):\n",
    "    qst=open('train_questions.txt', 'r')\n",
    "    cnt=open('train_contexts.txt', 'r')\n",
    "    # word = u\"الْعَرَبِيَّةُ\"\n",
    "\n",
    "    # if araby.is_arabicword(word):\n",
    "    #    print(\"valid arabic\")\n",
    "    # else: \n",
    "    #    print (\"invalid arabic word\")\n",
    "    # araby.strip_tashkeel(word)\n",
    "    # araby.strip_tatweel(word)\n",
    "    # print(word)\n",
    "    res=\"\"\n",
    "    question = qst.readlines()\n",
    "    for i in range(len(question)):\n",
    "       question[i].replace(\"\\n\",'')\n",
    "      #  print(question[i])\n",
    "       if userQuestion in question[i]:\n",
    "             cont=train_data['ctx'][i]\n",
    "            #  print(cont)\n",
    "             res=answer(userQuestion,cont)\n",
    "             break    \n",
    "    print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 697,
     "status": "ok",
     "timestamp": 1610467808775,
     "user": {
      "displayName": "rachid ahsoune",
      "photoUrl": "",
      "userId": "00239937024168847378"
     },
     "user_tz": -60
    },
    "id": "1f2Kg3faEesi",
    "outputId": "2cd6371e-7e2a-4d1f-b797-90dc43bf51b6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response(\"من كان يرافق طائرة يو؟\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_nh2LUQguo8L",
    "outputId": "eb10b87e-f977-4f50-eb56-a335dab7f8e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Running on http://95190124a3d2.ngrok.io\n",
      " * Traffic stats available on http://127.0.0.1:4040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [14/Jan/2021 15:13:21] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Jan/2021 15:13:22] \"\u001b[33mGET /static/js/index.js HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [14/Jan/2021 15:13:24] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [14/Jan/2021 15:15:09] \"\u001b[37mGET /get?msg=ما%20هو%20الموقع%20الذي%20أصبح%20مركزاً%20للأطباق%20الطائرة%20ونظريات%20المؤامرة؟ HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ما هو الموقع الذي أصبح مركزاً للأطباق الطائرة ونظريات المؤامرة؟\n",
      "منطقة 51\n",
      "منطقة 51\n",
      "منطقة 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [14/Jan/2021 15:15:32] \"\u001b[37mGET /get?msg=متى%20يقوم%20Qos%20بالتفاوض%20على%20كيفية%20عمل%20الشبكة؟ HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "متى يقوم Qos بالتفاوض على كيفية عمل الشبكة؟\n",
      "مرحلة إقامة الدورات.\n",
      "مرحلة إقامة الدورات.\n",
      "مرحلة إقامة الدورات.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [14/Jan/2021 15:16:03] \"\u001b[37mGET /get?msg=كم%20عدد%20قوائم%20الانتظار%20الموجودة%20على%20أجهزة%20توجيه%20المختلفة؟%5Cn HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "كم عدد قوائم الانتظار الموجودة على أجهزة توجيه المختلفة؟\\n\n",
      "متعددة\n",
      "متعددة\n",
      "متعددة\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [14/Jan/2021 15:16:19] \"\u001b[37mGET /get?msg=%20ما%20هو%20نوع%20العمل%20الذي%20يواجهه%20الطيارون%20العسكريون%20إذا%20انتقلوا%20إلى%20%5Cn%20مناطق%20محظورة؟%20 HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ما هو نوع العمل الذي يواجهه الطيارون العسكريون إذا انتقلوا إلى \\n مناطق محظورة؟ \n",
      "إجراءات التأديبية\n",
      "إجراءات التأديبية\n",
      "إجراءات التأديبية\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [14/Jan/2021 15:16:53] \"\u001b[37mGET /get?msg=مالذي%20يُظن%20بأنه%20قد%20تم%20بنائه%20في%20روزويل؟ HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مالذي يُظن بأنه قد تم بنائه في روزويل؟\n",
      "الطائرات على أساس التكنولوجيا\n",
      "الطائرات على أساس التكنولوجيا\n",
      "الطائرات على أساس التكنولوجيا\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [14/Jan/2021 15:17:29] \"\u001b[37mGET /get?msg=ما%20الذي%20جعل%20شريط%20الاختبار%20للطائرة؟ HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ما الذي جعل شريط الاختبار للطائرة؟\n",
      "قاع البحيرة\n",
      "قاع البحيرة\n",
      "قاع البحيرة\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [14/Jan/2021 15:18:24] \"\u001b[37mGET /get?msg=ما%20كان%20محور%20مؤامرة%20الجسم%20الغريب%20الحديثة؟%5Cn HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ما كان محور مؤامرة الجسم الغريب الحديثة؟\\n\n",
      "منطقة 51\n",
      "منطقة 51\n",
      "منطقة 51\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__,template_folder='/content/drive/MyDrive/Colab Notebooks/templates')\n",
    "run_with_ngrok(app)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route(\"/get\")\n",
    "def get_bot_reponse():\n",
    "    userText = request.args.get('msg')\n",
    "    print(userText)\n",
    "    print(str(response(userText)))\n",
    "    return str(response(userText))\n",
    "if __name__ == '__main__':\n",
    "  app.run()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Question_Answering.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
